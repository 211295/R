### 20/03/2024
### Regression models ###
### load packages:
if(!require(ape)){install.packages("ape");library(ape)}
if(!require(paleotree)){install.packages("paleotree");library(paleotree)}
if(!require(nlme)){install.packages("nlme");library(nlme)}
if(!require(caper)){install.packages("caper");library(caper)}

### set working directory load files
setwd("C:/Users/HP/Documents/FELIPE/Estatistica e Programacao/Analise Filogenetica")

### Since we already have time-scaled trees (see time-scaling), let's import them. For example, we can import the "mbl trees" we created with the previous script:
trees <- dget(file = "croc tree, 20 trees, mbl method.txt")

### selecting the first 5 trees only (instead of all 20 trees, for making analyses faster)
five_trees <- trees[1:5]

### load ages (as in time-scale tutorial)
ages <- read.table("croc_ages.txt", header=T, row.names = 1)

### load data (using function "read.table()":
my_data <- read.table("croc_sizes.txt", header=T) # avoid using the word "data"

### This dataset contains crocodylomorph body size data. Instead of estimating total body size, it uses a proxy (the cranial measurement ODCL)
### this measurement was log-transformed prior to being imported into R

### inspecting the 'my_data' object
my_data
class(my_data)
str(my_data) 
### this data has 241 observations (obs), which are the specimens, get the average values for each species

### let's do it using a loop and functions "unique()" and "mean()"
### function "unique()" gets unique names in element "Taxon"
taxon.list <- unique(my_data$Taxon) 
str(taxon.list) # we now know that there are 195 taxa
length(taxon.list)

### now, let's use a loop for getting mean values:
taxon.lengths <- c() 
### must create an empty list, R needs a object to receive the loop
for(i in 1: length(taxon.list)) {taxon.lengths[i] <- mean(my_data[my_data$Taxon==taxon.list[i], "log_odcl"])}
###this part can be a bit confusing, but it gets easy with practice

names(taxon.lengths) <- taxon.list
### uses the list of specimens we created above as the row names of this new list
my_new_data <- taxon.lengths # renaming it back to "my_data"

### inspecting, it should be 195 species!
my_new_data
head(my_new_data)
str(my_new_data)
class(my_new_data)
### it is now a 'numeric' object. we might need to convert it back to 'data frame' later...

### now, let's compare the tree with the data
length(my_new_data) # 195 taxa
Ntip(five_trees[[1]]) # 305 taxa

### Ok. So, now we need to drop (remove) taxa from the tree for which we do not have data

### again, we can do this using a loop and function "dropPaleoTip()":
### ps: there is another (more famous) function called "drop.tip()", which does basically the same,
### however, "dropPaleoTip()" works better on non-ultrametric trees (with fossils)
### because it keeps the ages of the nodes.
for(i in 1: length(five_trees)){five_trees[[i]] <- dropPaleoTip(five_trees[[i]], five_trees[[i]]$tip.label[!(five_trees[[i]]$tip.label %in% names(taxon.lengths))])}
### look after ->>> for(i in 1: length(five_trees)){five_trees[[i]] <- dropPaleoTip(treeOrig = five_trees[[i]],treeNew = five_trees[[i]]$tip.label[!(five_trees[[i]]$tip.label %in% names(taxon.lengths))])}

### comparing again:
length(my_new_data) # 195 taxa
Ntip(five_trees[[1]]) # 195 taxa

## Now, for the regression analyses, we will also need the "ages data" to contain the
### same number of taxa (195)

### originally, the 'ages" object has more taxa:
str(ages) #305 taxa

### So, let's drop (remove) the other taxa from it:
drop_ages <- ages[(rownames(ages) %in% five_trees[[1]]$tip.label), ]

### Let's compare all three objects:
length(my_new_data) # 195 taxa
Ntip(five_trees[[1]]) # 195 taxa
length(drop_ages$FAD) # 195 taxa

### Finally, let's transform the data back into a "data frame"
my_new_data2 <- data.frame(log_odcl = my_new_data, stringsAsFactors = FALSE)
class(my_new_data2)
str(my_new_data2)
### Dividing taxa into time bins

### In this case, we are using a time_bins file I created in Excel.
### It basically has some arbitrary divisions of time in ~9 million years, and extends back until de the end of the Triassic. In the end, there are 28 time bins

## Read time bins file
timebins <- read.table("time_bins.txt", header=F)

## time range (in binning scheme) of each taxon
taxarange <- binTimeData(drop_ages, int.times=timebins) # this is a function from "paleotree" package

# inspecting:
str(taxarange) # taxa are divided into 28 time bins
taxicDivDisc(taxarange) # this shows the "diversity through time", function of "paleotree" libery, use help to understand

# to know how many taxa (and which are they) for each of the 28 time bins:
taxon.times <- as.data.frame(taxarange$taxon.times)
print(taxon.times)
int.times <- as.data.frame(taxarange$int.times); colnames(int.times) <- c("first.int","last.int")
print(int.times)

### now, let's finally divide them into separate time bins
binned.taxa <- c() # creates an empty list for the loop function
for (i in 1:nrow(int.times)) {binned.taxa[[i]] <- taxon.times[taxon.times$first.int <= i & taxon.times$last.int >= i, ]}
print(binned.taxa[1]) # for taxa in the first (oldest) time bin
### No taxa in this time bin...

binned.taxa[3] #  for taxa in the third time bin 
### 4 taxa in this time bin.

binned.taxa[28] # for taxa in the 28th (earliest) time bin
### many taxa in this time bin!

### Now, let's set trait values (body size values) to each time bin
### body size values for all time bins:
values.all.bins<-c()
for (i in 1:nrow(int.times)) {values.all.bins[[i]] <- list(my_new_data2[rownames(binned.taxa[[i]]), "log_odcl"])}
names(values.all.bins)<-rownames(timebins)
values.all.bins

### For our regression analyses, we will use mean (average) values, let's obtain mean body size values for each time bin.
mean.all.bins<-c()
for (i in 1:nrow(int.times)) {mean.all.bins[[i]] <-mean(my_new_data2[rownames(binned.taxa[[i]]),"log_odcl"])}
names(mean.all.bins)<-rownames(timebins)
mean.all.bins

### Inspect 
head(mean.all.bins)
class(mean.all.bins)
str(mean.all.bins)

###  We could do the same for obtaining maximum, minimum or range values for each time bin it depends on the type of question we want to answer.

### Now, let's include all this information in a data frame (for making analyses easier):
mean_bins <- data.frame("log_odcl" = as.numeric(mean.all.bins), stringsAsFactors = FALSE)
rownames(mean_bins) <- c(28:1)
mean_bins

### visualizing:
plot(rownames(mean_bins), mean_bins$log_odcl, xlim=c(28,0), xlab="time bins", ylab="mean log_odcl", pch=16, col="blue")
### This allowed us to have an idea of the overall pattern of average body size of crocs through time
### pch is a cod of the shape of each point, 17 is triagle, 18 is a losang and 16 is circles

##############################################################################

### Importing abiotic (temperature) data

### The temperature data comes from Prokoph et al. (2008) and is represented by a proxy: delta O18
### the lowest the temperature, the highest the delta O18 value.
### temperature data from Prokoph et al. (2008) is divided into data collected from different latitudes.
### for our analyses, let's use data from temperate latitudes.

### Importing and visualising temperature data:
prokoph_temp <- read.table("Prokoph_temperature_temperate.txt")
### plotting:
plot(prokoph_temp[,1], prokoph_temp[,2],pch=16, col="blue", xlab="time Ma", ylab="d18O", xlim=c(230,0), ylim=c(-8,4))

### The temperature data was already subdivided into time bins
### values are weighted mean values for each time bin
prokoph_temp_binned <- read.table("Prokoph_temperature_bin.txt")
plot(prokoph_temp_binned[,1], prokoph_temp_binned[,2],pch=16, col="red", xlab="time bins", ylab="d18O", xlim=c(28,0), ylim=c(-3.5,1.5))
plot(prokoph_temp_binned[,1], prokoph_temp_binned[,2],pch=16, col="purple", xlab="time bins", ylab="d18O", xlim=c(28,0), ylim=c(-3.5,1), type = "l")

### Now, let's include temperature data into the data frame:
data_size_crocs <- data.frame("Average size" = mean_bins$log_odcl, "Temperature" = prokoph_temp_binned$V2, "oldest boundary" = int.times$first.int, "earliest boundary" = int.times$last.int)
print(data_size_crocs)
class(data_size_crocs)
str(data_size_crocs) # 28 objects (the )
### note that we don't have temperature data for the first 5 intervals, we will need to remove those first 5 before the regression analyses
### excluding lines (1 to 5) for which we don't have temperature or body size data
data_size_crocs <- data_size_crocs[-c(1:5),] # excludes first 5 lines
length(data_size_crocs[,1]) # 23 time bins

##############################################################################

### Regressions
### First, let's use OLS (the simplest regression), using function "lm()"
### this function uses a formula (y ~ x), where y is the dependent and x is the independent variable
### check the itens in data_size_crocs using $ + tab 1) Avarage.size | 2) Temperature | 3) oldest.boundary | 4) earlist.boundary
ols_model <- lm(data_size_crocs$Average.size ~ data_size_crocs$Temperature)
### let's use function "summary()" for looking into results
summary(ols_model)
head(ols_model)
class(ols_model)
str(ols_model)

### The summary table is important. It has all the information we need to report in publications.
### 1) the p-value: 0.5291. It is not significant | 2) the R squared: -0.02758. It's very weak | 3) value slope: 0.02285 | 4) the intercept: 2.07942
### Now, we can investigate the same relationship, but also incorporating an auto-regressive model,
### For time series analyses.
### We can do this using the "gls()" function, from "nlme" package:
gls_model <- gls(Average.size ~ Temperature, correlation = corARMA(p = 1), data = data_size_crocs, method = "ML")
summary(gls_model)

### let's look at the results 1) p-value: 0.7558. non-significant 2) slope: 0.0119928 3) intercept: 2.0671422
### Note that GLS regressions won't give us the R squared value.

### But it gives us a coefficient that tells us how strong the auto-correlation is 1) Phi: 0.4121824. moderate autoregressive correlation
## this moderate correlation might indicate that the regression with the auto regressive model
### is the best (when compared to the simple OLS model)

### To be sure about that, we need to compare the two models using the AIC scores

### For that, let's use the "gls()" function without any model, to obtain the AIC scores this should give us exactly the same results as the "lm()" function:

### the same but using gls() to get AIC (comparative)
ols_gls_model <- gls(Average.size ~ Temperature,data = data_size_crocs, method = "ML")
summary(ols_gls_model)

### Comparing AIC scores:
anova(ols_gls_model,gls_model)

### Indeed, the GLS with the auto-regressive model is "the best" fit (lower AIC score),
### even though both regressions indicated that the relationship between mean size and temperature
### is very weak.
### But the AIC scores allow us to report only the results of the GLS regression...

## Visualizing:
plot(data_size_crocs$Average.size ~ data_size_crocs$Temperature)
abline(ols_gls_model, col = "blue")
abline(gls_model, col = "red")

################################################################################################################
### Exercise number 1
### Use the same body size (mean_bins$log_odcl) and temperature data (prokoph_temp_binned$V2) and run correlation tests (Pearson, Spearman and/or Kendall)
### You can run correlation tests using the function 'cor.test()', see help("cor.test")
### SOLUTION:
pearson_model <- cor.test(prokoph_temp_binned$V2, mean_bins$log_odcl, method = "pearson", conf.level = 0.95, continuity = FALSE)
print(pearson_model)
### OR you can take the body size and the temperature thought the constructed data frame abouve (data_croc_size)
pearson_model2 = cor.test(data_size_crocs$Temperature, data_size_crocs$Average.size, method="pearson", conf.level=0.95, continuity = TRUE)
print(pearson_model2)
### It needs x (the independent variable; in this case, the temperature data) and y (the dependent variable; body size)
### It also uses the argument "method", which we can use for choosing between "pearson", "kendall" and "spearman"

### But before we we go ahead and run the tests, let's remember that the "Pearson" test assumes that your data is normally distributed
### To make sure our data follows a normal distribution, we need to test if the dataset follows a normal distribution, we can use function 'shapiro.test()'
### Let's perform the Shapiro test (normality test) on the body size data:
shapiro.test(data_size_crocs$Average.size)

### The null hypothesis of the Shapiro test is that the data is normally distributed.
### For the body size data, the p-value is 0.04928. This means that it rejects the null hypothesis. In other words, the data is NOT normally distributed
### Do the same for the temperature data:
shapiro.test(data_size_crocs$Temperature)
### The p-value is 0.6096, so it does not reject the null hypothesis (i.e. the data is normally distributed).

### But the Pearson correlation test requires that both variables follow a normal distribution
### So, we should not use Pearson in this case.

### Right, so let's run the tests with "kendall" and "spearman"
kendall_model <- cor.test(prokoph_temp_binned$V2, mean_bins$log_odcl, method = "kendall", conf.level = 0.95)
### OR
kendall_model2 <- cor.test(data_size_crocs$Temperature, data_size_crocs$Average.size, method = "kendall", conf.level=0.95)
print(kendall_model)

### p-value = 0.2483, so it's a non-significant relationship.
### But even if it was significant, the coefficient (tau) of  0.1778656 shows a very week correlation (close to zero)

### Let's do the same with "spearman":
spearman_model <- cor.test(prokoph_temp_binned$V2, mean_bins$log_odcl, method = "spearman", conf.level = 0.95)
### OR
spearman_model2 <- cor.test(data_size_crocs$Temperature, data_size_crocs$Average.size, method = "spearman", conf.level = 0.95)
print(spearman_model)
### As expected, similar results.
### Low correlation coefficient (rho): 0.277668
### And non-significant p-value: 0.1989

#############################################################################################################################
### Now, let's use PGLS, which incorporates phylogenetic information for that, let's import all the data again
### We will use the following objects (already loaded and checked previously):
my_new_data2
five_trees
drop_ages

### just double-checking if the number of taxa match
length(my_new_data2[,1]) # 195 taxa
Ntip(five_trees[[1]]) # 195 taxa - from ape library
length(drop_ages$FAD) # 195 taxa

### First, let's import the body size data again:
croc_size <- read.table("croc_sizes.txt", header=T)

### Now, let's also import a new file, with latitudinal information:
latitude <- read.table("Palaeolatitude.txt", header=T)
### See the infos we have in each file
head(latitude)
class(latitude)
summary(latitude)

### Let's put body size data and latitude in a same file:
### You have to ways to do that 1) creating a new column and attach the data there 2) use the function sum_cols 
ODCL_latitude <- croc_size
ODCL_latitude[,3] <- latitude$Lat
colnames(ODCL_latitude)[3] <- "Latitude"
### OR
ODCL_latitude2 = cbind(croc_size, latitude$Lat)
colnames(ODCL_latitude2)[3] <- "Latitude"
### OR
ODCL_latitue3 = cbind(latitude, croc_size$log_odcl)
colnames(ODCL_latitue3)[3] <- "OQEISSO?"

print(ODCL_latitude)
print(ODCL_latitude2)
print(ODCL_latitue3)

### We already have the 5 time-calibrated trees we need:
five_trees
### But, for now, let's select just one tree for PGLS (tree number 1 for example):
pgls_tree <- five_trees[[1]]

### calculating the mean value for multiple specimens
taxon.list <- unique(ODCL_latitude$Taxon)
taxon.lengths<-c()
for(i in 1:length(taxon.list)) {taxon.lengths[i]<-mean(ODCL_latitude[ODCL_latitude$Taxon==taxon.list[i],
                                                                     "log_odcl"])}
names(taxon.lengths)<-taxon.list
log_ODCL<-taxon.lengths

### calculating the mean latitude for multiple specimens
taxon.list.lat <- unique(ODCL_latitude$Taxon)
taxon.lengths.lat<-c()
for(i in 1:length(taxon.list.lat)) {taxon.lengths.lat[i]<-mean(ODCL_latitude[ODCL_latitude$Taxon==taxon.list.lat[i],
                                                                             "Latitude"])}
names(taxon.lengths.lat)<-taxon.list.lat
Lat<-taxon.lengths.lat

### everything in a same data frame again:
all_data <-data.frame("Taxon" = names(log_ODCL), "log_ODCL" = unname(log_ODCL, force = F), "Lat" = unname(Lat, force = F))
all_data

## Let's plot body size against latitude (to look for any pattern):
plot(all_data$log_ODCL ~ all_data$Lat, xlab = "Latitude", ylab="log_ODCL", pch=18)

### It doesn't look very promising...

##############################################################################
### Now, the regressions:
### First, the simple Linear Model OLS using the function lm():
ols_model_lat<-lm(log_ODCL ~ Lat, data = ODCL_latitude)
summary(ols_model_lat)

### It shows an almost significant correlation (p-value = 0.05985), but weak (R squared = 0.01314)

### Now, the PGLS, that incorporates the phylogeny, for that, let's use the "pgls()" function, from the "caper" package.
### 1) put the data in the right format for the function | 2) the pgls() function:
new_ODCL_latitude <- comparative.data(pgls_tree, all_data, Taxon, vcv=T, vcv.dim=3)
pgls_model <- pgls(log_ODCL ~ Lat, new_ODCL_latitude, lambda= "ML")
summary(pgls_model)

### This function also gives us a R squared (which can be useful, but problematic), in this case, the correlation is significant (p-value = 0.03048), but still weak (R squared = 0.0316)
### If we want to compare both regressions, we need, once more, to get the AIC for the OLS using the gls() function:
ols_gls_model_lat<- gls(log_ODCL ~ Lat, data = all_data, method = "ML")
summary_ols<-summary(ols_gls_model_lat)

### We cannot use the "anova()" function
### but we can get the AIC scores for both regressions:
### OLS
print(summary_ols$AIC)

### PGLS
print(pgls_model$aic)

### The PGLS fits better our data (lower AIC scores), and we can report the results from this regression only.
### Visualizing:
plot(all_data$log_ODCL ~ all_data$Lat, xlab="Latitude", ylab="log_ODCL", pch=16, col ="gray")
abline(ols_gls_model_lat, col = "blue")
abline(pgls_model, col = "red")

##############################################################################
### Exercise number 2
### Use data loaded below, evaluate the relationship between ecology (habitat) and body size in croodylomorphs.
### Read the body size data
data_habitats<-read.table("croc_size_habitats.txt",header=T)

### let's quickly look at the data using boxplots
boxplot(log_ODCL ~ Aquaticness, data = data_habitats, col="lightgreen")

### Aquatic species look larger on average than the other two ecologies/habitats. But is it significant?
### Investigate this using a phylogenetic ANOVA (there are multiple functions/packages that can do this)

### SOLUTION:
### First, let's import the trees and remove taxa for which we don't have data
### if you already have the trees (or one tree), you don't need to import them and/or remove the taxa)

### loading tree:
trees<-dget(file = "croc tree, 20 trees, mbl method.txt")
### removing taxa, using a :
for (i in 1: length(trees)) {trees[[i]] <- dropPaleoTip(trees[[i]], trees[[i]]$tip.label[!(trees[[i]]$tip.label %in% data_habitats$Taxon)])}
### let's select only one tree to make it faster, but you can do for every tree:
tree_number1 <- trees[[1]]
print(tree_number1)

### Now, we can run a phylogenetic ANOVA using different functions, from different packages the most used is the function 'phylANOVA()' from the package "phytools". So, let's load package phytools
if(!require(phytools)){install.packages("phytools");library(phytools)}

### Now, let's try to use function 'phylANOVA()'. If you search for this function in the "Help" tab, it will explain what you need.
### 1) A tree (already have it) | 2) The 'x' is "a vector containing the group" (this is the "aquaticness" of the species) | 3) The 'y' is "a vector containing the response variable" (this is the body size data)
### 4) The number of simulations (nsim) is usually more than 1000, but can use only 100 to make it faster | 5) A method to adjust (i.e. correct) the p-value. The "Bonferroni" method is good choice.
### Let's try!
anova_phy <- phylANOVA(tree_number1, data_habitats$Aquaticness, data_habitats$log_ODCL, nsim = 100, p.adj = "bonferroni")
### It worked, but there were warning messages:
### "Warning: no labels for x/y. Assuming order of tree$tip.label.
### This warning indicates that there is something wrong with our groups ("aquaticness") and data (body size data)

### Let's take a look at them:
print(data_habitats$Aquaticness)
print(data_habitats$log_ODCL)

### The problem is that they don't show the species/taxa names, so the function don't know how to match these with the species in the tree
### To solve this, we need to add the species names to the groups and data. We can do this creating new objects for the groups and data
### But another important thing to do is double checking what the documentation of the package says in the "Help" tab
### For both the groups ('x') and data ('y') it says that they have to be a vector. So, to make sure we are creating vectors, let's use the function 'as.vector()':
aquaticness_vector <- as.vector(data_habitats$Aquaticness)
log_ODCL_vector <- as.vector(data_habitats$log_ODCL)

### inspect:
class(aquaticness_vector)
str(aquaticness_vector)
head(aquaticness_vector)
summary(aquaticness_vector)

### Right, now we are sure that these two objects are vectors (as the function requires), but they still don't have the species names
### We know that the first column (called "Taxon") of the 'data_habitats' object has the species names. And this first column is called "Taxon". We can check:
head(data_habitats)

### So, let's add the names (from data_habitats$Taxon) using the function 'names()':
### And we
names(aquaticness_vector) <- data_habitats$Taxon
names(log_ODCL_vector) <- data_habitats$Taxon

### Now it should work:
anova_phy  <- phylANOVA(tree_number1, aquaticness_vector, log_ODCL_vector, nsim = 100, p.adj = "bonferroni")

### IT WORKED!
### let's check the results:
anova_phy # looks like none of the ecological groups are significantly different from one another

### There's another option for doing it, which is the function 'lm.rrpp()' from the package "RRPP"
### SO, let's load "RRPP" (if you don't have this package, install it using the "Packages" tab):
if(!require(RRPP)){install.packages("RRPP");library(RRPP)}
### Use "Help" tab for the function 'lm.rrpp()'

### It needs a formula and the number of iterations/simulations, but what is the formula?
### The formula is usually "dependent variable (y) ~ independent variable (x)", in our case, the formula should be "log_ODCL_vector ~ aquaticness_vector". For the number of simulations, let's use 100 again
### Finally, where is the phylogenetic information? We need to incorporate the phylogenetic information using argument "Cov", the covariance matrix
phylo_anova_lm.rrpp  <- lm.rrpp(log_ODCL_vector ~ aquaticness_vector, Cov = vcv.phylo(tree_number1), iter = 100)

### It seems like it worked!
### Let's inspect the results:
head(phylo_anova_lm.rrpp) # Cool, but this is not what we need.
summary(phylo_anova_lm.rrpp) # still not exactly what we need...

### We need pairwise comparisons, to be sure that the groups are or not significantly different from one another
### so, let's use "summary()" in combination with "pairwise()" (another function from the same package)
summary(pairwise(phylo_anova_lm.rrpp, groups = aquaticness_vector), test.type = "dist", show.vectors = TRUE)


### Well, this shows different results! Looks like terrestrial crocs are significantly smaller than both aquatic and semi-aquatic ones
### Function 'lm.rrpp()' uses a different method (permutation procedure) from that of phylANOVA (simulations). You can read more about it in
### Adams DC & Collyer ML (2018) Phylogenetic ANOVA: Group-clade aggregation, biological challenges, and a refined permutation procedure. Evolution, https://doi.org/10.1111/evo.13492.
### And
### Collyer ML & Adams DC (2018) RRPP: An r package for fitting linear models to high‐dimensional data, using residual randomization. Methods in Ecology and Evolution, https://doi.org/10.1111/2041-210X.13029.
